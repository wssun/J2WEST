<!DOCTYPE html>
<html>

<head>
  <!-- Basic -->
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <!-- Mobile Metas -->
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
  <!-- Site Metas -->
  <meta name="keywords" content="" />
  <meta name="description" content="" />
  <meta name="author" content="" />

  <title>Security of LM4Code</title>

  <!-- slider stylesheet -->
  <!-- slider stylesheet -->
  <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/OwlCarousel2/2.3.4/assets/owl.carousel.min.css" />

  <!-- bootstrap core css -->
  <link rel="stylesheet" type="text/css" href="htmls/css/bootstrap.css" />

  <!-- fonts style -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:400,700|Poppins:400,700&display=swap" rel="stylesheet">
  <!-- Custom styles for this template -->
  <link href="htmls/css/style.css" rel="stylesheet" />
  <!-- responsive style -->
  <link href="htmls/css/responsive.css" rel="stylesheet" />
</head>

<body>
  <div class="hero_area">
    <!-- header section strats -->
    <header class="header_section">
      <div class="container-fluid">
        <nav class="navbar navbar-expand-lg custom_nav-container pt-3">
          <a class="navbar-brand" href="index.html">
            <span>
              Security of LM4Code
            </span>
          </a>
          <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
          </button>

          <div class="collapse navbar-collapse" id="navbarSupportedContent">
            <div class="d-flex ml-auto flex-column flex-lg-row align-items-center">
              <ul class="navbar-nav  ">
                <li class="nav-item active">
                  <a class="nav-link" href="index.html">Home <span class="sr-only">(current)</span></a>
                </li>
                <!-- <li class="nav-item">
                  <a class="nav-link" href="about.html"> About </a>
                </li>
                <li class="nav-item">
                  <a class="nav-link" href="do.html"> What we do </a>
                </li>
                <li class="nav-item">
                  <a class="nav-link" href="portfolio.html"> Portfolio </a>
                </li>
                <li class="nav-item">
                  <a class="nav-link" href="contact.html">Contact us</a>
                </li> -->
              </ul>
              <div class="user_option">
                <a href="htmls/member.html">
                  <img src="htmls/images/user.png" alt="" style="height: 25px;">
                </a>
                <!-- <form class="form-inline my-2 my-lg-0 ml-0 ml-lg-4 mb-3 mb-lg-0">
                  <button class="btn  my-2 my-sm-0 nav_search-btn" type="submit"></button>
                </form> -->
              </div>
            </div>
          </div>
        </nav>
      </div>
    </header>
    <!-- end header section -->
    <!-- slider section -->
    <section class=" slider_section position-relative">
      <div class="container">
        <div id="carouselExampleIndicators" class="carousel slide" data-ride="carousel">
          <ol class="carousel-indicators">
            <li data-target="#carouselExampleIndicators" data-slide-to="0" class="active"></li>
            <li data-target="#carouselExampleIndicators" data-slide-to="1"></li>
            <li data-target="#carouselExampleIndicators" data-slide-to="2"></li>
          </ol>
          <div class="carousel-inner">
            <div class="carousel-item active">
              <div class="row">
                <div class="col">
                  <div class="detail-box">
                    <div>
                      <h2>
                        welcome to
                      </h2>
                      <h1>
                        Security of LM4Code
                      </h1>
                      <p>
                        &nbsp;&nbsp;&nbsp;&nbsp;Deep learning techniques have revolutionized software engineering (SE), especially code-related tasks involving programming language understanding and generation. Deep neural models for code-related tasks
                        are collectively referred to as neural code models (NCMs). NCMs demonstrate advantages in numerous
                        code-related tasks that traditional methods and machine learning approaches cannot rival. Although NCMs
                        have demonstrated great potential in solving code-related tasks, their strong performance has also recently
                        attracted the attention of (malicious) attackers from the SE and security communities. 
                      </p>
                      <!-- <div class="">
                        <a href="">
                          Contact us
                        </a>
                      </div> -->
                    </div>
                  </div>
                </div>
              </div>
            </div>
            <div class="carousel-item">
              <div class="row">
                <div class="col">
                  <div class="detail-box">
                    <div>
                      <h2>
                        welcome to

                      </h2>
                      <h1>
                        Security of LM4Code
                      </h1>
                      <p>
                        &nbsp;&nbsp;&nbsp;&nbsp;In essence, the nature
                        and architecture of NCMs are also neural networks, so they also inherit the vulnerability of neural networks.
                        The security of NCMs has become a focal point, with a growing number of methods proposed for attacks
                        and defenses against NCMs. However, a systematic review of these attacks and defenses is currently lacking, hindering future researchers from understanding the research status, development trends, challenges,
                        and opportunities in the field of NCM security.
                      </p>
                      <!-- <div class="">
                        <a href="">
                          Contact us
                        </a>
                      </div> -->
                    </div>
                  </div>
                </div>
              </div>
            </div>
            <div class="carousel-item">
              <div class="row">
                <div class="col">
                  <div class="detail-box">
                    <div>
                      <h2>
                        welcome to

                      </h2>
                      <h1>
                        Security of LM4Code
                      </h1>
                      <p>
                        &nbsp;&nbsp;&nbsp;&nbsp;In this paper, we present a comprehensive and systematic
                        survey of research on NCM security. Specifically, we first employ a systematic approach to retrieve relevant
                        research literature from the SE, security, and artificial intelligence (AI) communities, and finally carefully select
                        representative papers from an initial pool of candidates. Second, after reviewing
                        these papers, we provide an overview of the security threats faced by NCMs and categorize these threats
                        into backdoor attacks, adversarial attacks, and other attacks based on the methods of their implementation.
                        Third, for each category, we depict the general workflow and summarize existing attacks as well as defenses.
                        Moreover, we also scrutinize the widely used datasets, evaluation metrics, and replication packages utilized in
                        NCM security studies. Finally, we outline key challenges and potential future research opportunities, aiming
                        to provide useful guidance for subsequent researchers to further advance the security of NCMs.
                      </p>
                      <!-- <div class="">
                        <a href="">
                          Contact us
                        </a>
                      </div> -->
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>

      </div>
    </section>
    <!-- end slider section -->
  </div>

  <!-- do section -->

  <section class="do_section layout_padding">
    <div class="container">
      <div class="heading_container">
        <h2>
          Paper Classification
        </h2>
      </div>
      <div class="do_container">
        <div class="box arrow-start arrow_bg">
          <div class="img-box">
              <img src="htmls/images/d-1.png" alt="">
          </div>
          <div class="detail-box">
            <!-- <h6>
              Paper Class One
            </h6> -->
            <a href="htmls/Adversarial Attacks.html" style="color: rgb(1, 1, 11); text-decoration: none; font-size: 16px; font-weight: bold;">
              Adversarial Attacks
            </a>
          </div>
        </div>

        <div class="box arrow-middle arrow_bg">
          <div class="img-box">
            <img src="htmls/images/d-2.png" alt="">
          </div>
          <div class="detail-box">
            <a href="htmls/Backdoor Attacks.html" style="color: rgb(1, 1, 26); text-decoration: none; font-size: 16px; font-weight: bold;">
              Backdoor Attacks
            </a>
          </div>
        </div>

        <div class="box arrow-middle arrow_bg">
          <div class="img-box">
            <img src="htmls/images/d-3.png" alt="">
          </div>
          <div class="detail-box">
            <a href="htmls/Other Attacks.html" style="color: rgb(1, 1, 14); text-decoration: none; font-size: 16px; font-weight: bold;">
              Other Attacks
            </a>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- end do section -->

  <!-- client section -->

  <section class="client_section">
    <div class="container">
      <div class="heading_container">
        <h2>
          RENCENT PAPERS
        </h2>
      </div>
      <div class="carousel-wrap ">
        <div class="owl-carousel">
          <div class="item">
            <div class="box">
              <div class="img-box">
                <a href="https://arxiv.org/pdf/2304.13255"><img src="htmls/images/recent_papers/shield.png" alt=""></a>
              </div>
              <div class="detail-box">
                <h5>
                  SHIELD: Thwarting Code Authorship Attribution
                </h5>
                <!-- <img src="htmls/images/quote.png" alt=""> -->
                <p>
                  Authorship attribution has become increasingly accurate, posing a serious privacy risk for programmers who wish
                  to remain anonymous. In this paper, we introduce SHIELD to
                  examine the robustness of different code authorship attribution
                  approaches against adversarial code examples. We define four
                  attacks on attribution techniques, which include targeted and
                  non-targeted attacks, and realize them using adversarial code
                  perturbation. We experiment with a dataset of 200 programmers
                  from the Google Code Jam competition to validate our methods
                  targeting six state-of-the-art authorship attribution methods that
                  adopt a variety of techniques for extracting authorship traits from
                  source-code, including RNN, CNN, and code stylometry. Our
                  experiments demonstrate the vulnerability of current authorship
                  attribution methods against adversarial attacks. For the nontargeted attack, our experiments demonstrate the vulnerability
                  of current authorship attribution methods against the attack
                  with an attack success rate exceeds 98.5% accompanied by a
                  degradation of the identification confidence that exceeds 13%.
                  For the targeted attacks, we show the possibility of impersonating
                  a programmer using targeted-adversarial perturbations with a
                  success rate ranging from 66% to 88% for different authorship
                  attribution techniques under several adversarial scenarios.
                </p>
              </div>
            </div>
          </div>
          <div class="item">
            <div class="box">
              <div class="img-box">
                <a href="https://arxiv.org/pdf/2301.02496"><img src="htmls/images/recent_papers/yangzhou.png" alt=""></a>
              </div>
              <div class="detail-box">
                <h5>
                  Stealthy Backdoor Attack for Code Models.
                </h5>
                <!-- <img src="htmls/images/quote.png" alt=""> -->
                <p>
                  Code models, such as CodeBERT and CodeT5, offer general-purpose representations of code and play a vital role in
                  supporting downstream automated software engineering tasks. Most recently, code models were revealed to be vulnerable to backdoor
                  attacks. A code model that is backdoor-attacked can behave normally on clean examples but will produce pre-defined malicious
                  outputs on examples injected with triggers that activate the backdoors. Existing backdoor attacks on code models use unstealthy and
                  easy-to-detect triggers. This paper aims to investigate the vulnerability of code models with stealthy backdoor attacks. To this end, we
                  propose AFRAIDOOR (Adversarial Feature as Adaptive Backdoor). AFRAIDOOR achieves stealthiness by leveraging adversarial
                  perturbations to inject adaptive triggers into different inputs. We apply AFRAIDOOR to three widely adopted code models (CodeBERT,
                  PLBART and CodeT5) and two downstream tasks (code summarization and method name prediction). We evaluate three widely used
                  defense methods and find that AFRAIDOOR is much likely to be detected by the defense methods than baseline methods. More
                  specifically, when using spectral signature as defense, around 85% of adaptive triggers in AFRAIDOOR bypass the detection in the
                  defense process. By contrast, only less than 12% of the triggers from previous work bypass the defense. When the defense method is
                  not applied, both AFRAIDOOR and baselines have almost perfect attack success rates. However, once a defense is applied, the attack
                  success rates of baselines decrease dramatically, while the success rate of AFRAIDOOR still remains high. Our finding exposes security
                  weaknesses in code models under stealthy backdoor attacks and shows that the state-of-the-art defense method cannot provide
                  sufficient protection. We call for more research efforts in understanding security threats to code models and developing more effective
                  countermeasures.
                </p>
              </div>
            </div>
          </div>
          <div class="item">
            <div class="box">
              <div class="img-box">
                <a href="https://arxiv.org/pdf/2301.02412"><img src="htmls/images/recent_papers/CODA.png" alt=""></a>
              </div>
              <div class="detail-box">
                <h5>
                  Code Difference Guided Adversarial Example Generation for Deep Code Models
                </h5>
                <!-- <img src="htmls/images/quote.png" alt=""> -->
                <p>
                  Adversarial examples are important to test and enhance the robustness of deep code models. As source code is discrete and has to strictly stick to complex grammar and semantics constraints, the adversarial example generation techniques in other domains are hardly applicable. Moreover, the adversarial example generation techniques specific to deep code models still suffer from unsatisfactory effectiveness due to the enormous ingredient search space. In this work, we propose a novel adversarial example generation technique (i.e., CODA) for testing deep code models. Its key idea is to use code differences between the target input (i.e., a given code snippet as the model input) and reference inputs (i.e., the inputs that have small code differences but different prediction results with the target input) to guide the generation of adversarial examples. It considers both structure differences and identifier differences to preserve the original semantics. Hence, the ingredient search space can be largely reduced as the one constituted by the two kinds of code differences, and thus the testing process can be improved by designing and guiding corresponding equivalent structure transformations and identifier renaming transformations. Our experiments on 15 deep code models demonstrate the effectiveness and efficiency of CODA, the naturalness of its generated examples, and its capability of enhancing model robustness after adversarial fine-tuning. For example, CODA reveals 88.05% and 72.51% more faults in models than the state-of-the-art techniques (i.e., CARROT and ALERT) on average, respectively.
                </p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- footer section -->
  <section class="container-fluid footer_section">
    <p>
      &copy; 2024 All Rights Reserved By
      <a href="https://html.design/">Free Html Templates</a>
    </p>
  </section>
  <!-- footer section -->

  <script type="text/javascript" src="htmls/js/jquery-3.4.1.min.js"></script>
  <script type="text/javascript" src="htmls/js/bootstrap.js"></script>
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/OwlCarousel2/2.3.4/owl.carousel.min.js">
  </script>
  <!-- owl carousel script 
    -->
  <script type="text/javascript">
    $(".owl-carousel").owlCarousel({
      loop: true,
      margin: 0,
      navText: [],
      center: true,
      autoplay: true,
      autoplayHoverPause: true,
      responsive: {
        0: {
          items: 1
        },
        1000: {
          items: 3
        }
      }
    });
  </script>
  <!-- end owl carousel script -->

</body>

</html>