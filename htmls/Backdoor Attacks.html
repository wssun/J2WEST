<!DOCTYPE html>
<html>

<head>
  <!-- Basic -->
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <!-- Mobile Metas -->
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
  <!-- Site Metas -->
  <meta name="keywords" content="" />
  <meta name="description" content="" />
  <meta name="author" content="" />

  <title>Adversarial Attacks</title>

  <!-- slider stylesheet -->
  <!-- slider stylesheet -->
  <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/OwlCarousel2/2.3.4/assets/owl.carousel.min.css" />

  <!-- bootstrap core css -->
  <link rel="stylesheet" type="text/css" href="css/bootstrap.css" />

  <!-- fonts style -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:400,700|Poppins:400,700&display=swap" rel="stylesheet">
  <!-- Custom styles for this template -->
  <link href="css/style.css" rel="stylesheet" />
  <!-- responsive style -->
  <link href="css/responsive.css" rel="stylesheet" />

  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">
</head>

<body>

  <div class="hero_area">
    <!-- header section strats -->
    <header class="header_section">
      <div class="container-fluid">
        <nav class="navbar navbar-expand-lg custom_nav-container pt-3">
          <a class="navbar-brand" href="../index.html">
            <span>
              Adversarial Attacks
            </span>
          </a>
          <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
          </button>

          <div class="collapse navbar-collapse" id="navbarSupportedContent">
            <div class="d-flex ml-auto flex-column flex-lg-row align-items-center">
              <ul class="navbar-nav  ">
                <li class="nav-item active">
                  <a class="nav-link" href="../index.html">Home <span class="sr-only">(current)</span></a>
                </li>
                <!-- <li class="nav-item">
                  <a class="nav-link" href="about.html"> About </a>
                </li> -->
                <!-- <li class="nav-item">
                  <a class="nav-link" href="do.html"> What we do </a>
                </li>
                <li class="nav-item">
                  <a class="nav-link" href="portfolio.html"> Portfolio </a>
                </li> -->
                <!-- <li class="nav-item">
                  <a class="nav-link" href="contact.html">Contact us</a>
                </li> -->
              </ul>
              <div class="user_option">
                <a href="">
                  <img src="images/businessman.svg" alt="" style="height: 25px;">
                </a>
                <!-- <form class="form-inline my-2 my-lg-0 ml-0 ml-lg-4 mb-3 mb-lg-0">
                  <button class="btn  my-2 my-sm-0 nav_search-btn" type="submit"></button>
                </form> -->
              </div>
            </div>
          </div>
        </nav>
      </div>
    </header>
    <!-- end header section -->
    <!-- slider section -->
    <section class=" slider_section position-relative">
      <div class="container">
        <!-- <div id="carouselExampleIndicators" class="carousel slide" data-ride="carousel">
          <ol class="carousel-indicators"> 
            <li data-target="#carouselExampleIndicators" data-slide-to="0" class="active"></li>
            <li data-target="#carouselExampleIndicators" data-slide-to="1"></li>
            <li data-target="#carouselExampleIndicators" data-slide-to="2"></li>
          </ol>  -->

          <div class="carousel-inner">
            <div class="carousel-item active">
              <div class="row">
                <div class="col">
                  <div class="detail-box">
                    <div>
                      <h2>
                        welcome to

                      </h2>
                      <h1>
                        Backdoor Attacks
                      </h1>
                      <!-- <p>
                        一段关于后门攻击的介绍
                      </p> -->

                      <!-- <div class="">
                        <a href="">
                          Contact us
                        </a>
                      </div> -->

                    </div>
                  </div>
                </div>
              </div>
            </div>

            <!-- <div class="carousel-item">
              <div class="row">
                <div class="col">
                  <div class="detail-box">
                    <div>
                      <h2>
                        welcome to

                      </h2>
                      <h1>
                        AI Security
                      </h1>
                      <p>
                        这里是一段总述的介绍文字Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut
                        labore
                      </p>
                      <div class="">
                        <a href="">
                          Contact us
                        </a>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div> -->

            <!-- <div class="carousel-item">
              <div class="row">
                <div class="col">
                  <div class="detail-box">
                    <div>
                      <h2>
                        welcome to

                      </h2>
                      <h1>
                        AI Security
                      </h1>
                      <p>
                        这里是一段总述的介绍文字Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut
                        labore
                      </p>
                      <div class="">
                        <a href="">
                          Contact us
                        </a>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div> -->

          </div>
        </div>

      </div>
    </section>
    <!-- end slider section -->
  </div>


  <!-- work section -->
  <section class="work_section layout_padding">
    <div class="container">
      <div class="heading_container">
        <h2>
          Introduction
        </h2>
        <p>
          <h4>White-box Attacks </h4> 
          In white-box adversarial attacks, attackers have complete knowledge of
          the target model, including model structure, weight parameters, and training data. In this scenario,
          attackers can directly access the internal information of the target model, making it easier to
          understand the model’s characteristics and vulnerabilities. Attackers can generate adversarial
          examples in a targeted manner by analyzing model gradients, loss functions, and other information,
          causing the model to produce misleading outputs. White-box attacks typically involve using gradient
          information for backpropagation to maximize changes in input, steering the model output toward
          the direction expected by the attacker. With carefully designed adversarial examples, attackers
          can guide the model to make incorrect decisions, posing significant harm in practical applications
          <br>
          <br>
          <h4>Black-box Attacks </h4> 
          In recent years, black-box adversarial attacks in the field of neural code
          models have been widely studied. In contrast to white-box adversarial attacks, where the attacker
          has detailed information about the model’s structure and weights, black-box adversarial attacks
          involve attackers who cannot access such detailed information. In black-box attacks, adversaries
          can only generate adversarial examples by obtaining limited model outputs through model queries.
          The harm caused by black-box attacks primarily manifests in compromised model performance and
          threats to system security. In situations where detailed model information is unavailable, attackers
          ingeniously construct adversarial examples, potentially leading to misleading outputs from neural
          code models, affecting the accuracy of the model in practical tasks. This not only poses a potential
          threat to downstream models in software engineering tasks but may also result in serious issues in
          security-critical systems.
        </p>
        <!-- <h2>
          RENCENT WORKS
        </h2>
        <p>
          Figure1(upper left corner): 《SHIELD: Thwarting Code Authorship Attribution》In 2023, Abuhamad et al. [3] introduce SHIELD to attack the code authorship attribution system,
          demonstrating the insufficient robustness of current code authorship attribution systems. SHIELD
          injects carefully selected code samples into the target source code and then applies a code-to-code
          obfuscator [] for obfuscation. SHIELD employs obfuscation to make it challenging for models to
          statically identify or remove the injected code portions, rather than solely conceal the identity of the
          code author. Adversarial samples are inputted into a black-box code authorship attribution system,
          and then the probability distribution of the identification model is modified by SHIELD through
          altering the code expressions and statements of the adversarial samples. SHIELD achieves a success
          rate of over 95.5% in non-targeted attacks, with a decrease in model identification confidence of
          over 13%. In targeted attacks, SHIELD achieves an attack success rate of over 66%.This indicates
          that the current authorship attribution system is highly susceptible to the influence of adversarial
          samples.
        </p> -->
      </div>
      <!-- <div class="work_container layout_padding2">
        <div class="box b-1">
          <img src="images/class1_papers/SHIELD.png" alt="">
        </div>
        <div class="box b-2">
          <img src="images/class1_papers/paper2.png" alt="">
        </div>
        <div class="box b-3">
          <img src="images/class1_papers/paper3.png" alt="">

        </div>
        <div class="box b-4">
          <img src="images/class1_papers/paper1.png" alt="">
        </div>
      </div> -->
    </div>
  </section>

  <section>
    <div class="container mt-5">
      <h2>Datasets used in adversarial research on NCMs</h2>
      <table class="table table-bordered table-striped table-hover">
        <thead class="thead-dark">
          <tr>
            <th scope="col">Dataset</th>
            <th scope="col">Year</th>
            <th scope="col">Programming Language</th>
            <th scope="col">Data Source</th>
            <th scope="col">Download Link</th>
            <th scope="col">Study</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>BigCloneBench</td>
            <td>2014</td>
            <td>Java</td>
            <td>GitHub</td>
            <td><a href="https://github.com/clonebench/BigCloneBench">Download</a></td>
            <td>XXX</td>
          </tr>
          <!-- 其他行数据 -->
          <!-- ... -->
          <tr>
            <td>OJ dataset</td>
            <td>2016</td>
            <td>C++</td>
            <td> OJ Platform</td>
            <td> <a href="http://programming.grids.cn">Download</a></td>
            <td>XXX</td>
          </tr>

          <tr>
            <td>CodeSearchNet</td>
            <td>2019</td>
            <td>Go <br>
                Java <br>
                JavaScript <br>
                PHP <br>
                Python <br>
                Ruby <br>
            </td>
            <td> GitHub </td>
            <td> <a href=" https://github.com/github/CodeSearchNet">Download</a></td>
            <td>XXX</td>
          </tr>


        </tbody>
      </table>
    </div>
  </section>

  <section>
    <div class="container mt-5">
      <h2> A summary of existing adversarial attacks in NCMs</h2>
      <table class="table table-bordered table-striped table-hover">
        <thead class="thead-dark">
          <tr>
            <th scope="col">Attack Technique</th>
            <th scope="col">Year</th>
            <th scope="col">Venue</th>
            <th scope="col">Attack Type</th>
            <th scope="col">Target Models</th>
            <th scope="col">Target Tasks</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Quiring et al.</td>
            <td>2019</td>
            <td>USENIX Security</td>
            <td>Black-box Attack</td>
            <td>Random Forest <br>
                LSTM</td>
            <td>Authorship Attribution</td>
          </tr>
          <!-- 其他行数据 -->
          <!-- ... -->
          <tr>
            <td>DAMP</td>
            <td>2020</td>
            <td>OOPSLA</td>
            <td>White-box Attack</td>
            <td>Code2Ve <br>
                GGNN</td>
            <td>Method Name Prediction <br>
                Variable Name Prediction</td>
          </tr>

          <tr>
            <td>STRATA</td>
            <td>2020</td>
            <td>Arxiv</td>
            <td>Black-box Attack</td>
            <td>Code2Seq</td>
            <td>Method Name Prediction</td>
          </tr>

          <tr>
            <td>MHM</td>
            <td>2020</td>
            <td>AAAI</td>
            <td>Black-box Attack</td>
            <td>BiLSTM <br>
                ASTNN
            </td>
            <td>Function Classification</td>
          </tr>
          
          <tr>
            <td>Srikant et al.</td>
            <td>2021</td>
            <td>ICLR</td>
            <td>White-box Attack</td>
            <td>Seq2Seq</td>
            <td>Method Name Prediction</td>
          </tr>




        </tbody>
      </table>
    </div>
  </section>

  <!-- <section class="work_section layout_padding">
    <div class="container">
      <div class="heading_container">
        <h2>
          Papers
        </h2>
      </div>
    </div>
  </section> -->

  <!-- end work section -->
  <section class="who_section">
    
    <div class="container">

      <div class="row">
        <div class="col-md-12">
            <!-- 内容展示区域 -->
            <div id="contentArea" class="content-item">
                <!-- 动态加载内容 -->
            </div>
        </div>
      </div>


        <!-- <div class="row">
          <div class="col-md-5">
            <div class="img-box">
              <img src="images/class1_papers/paper1.png" alt="">
            </div>
          </div>
          <div class="col-md-7">
            <div class="detail-box">
              <div class="heading_container">
                <h2>
                  Data-efficient Fine-tuning for LLM-based Recommendation
                </h2>
              </div>
              <p>
                <h3> Authors:  </h3>
                Xinyin Ma, Gongfan Fang, Xinchao Wang
              </p>

              <p>
                Large language models (LLMs) have shown remarkable capabilities in language 
                understanding and generation. However, such impressive capability typically 
                comes with a substantial model size, which presents significant challenges 
                in both the deployment, inference, and training stages. With LLM being a 
                general-purpose task solver, we explore its compression in a task-agnostic 
                manner, which aims to preserve the multi-task solving and language generation 
                ability of the original LLM. One challenge to achieving this is the enormous 
                size of the training corpus of LLM, which makes both data transfer and model 
                post-training over-burdensome. Thus, we tackle the compression of LLMs within 
                the bound of two constraints: being task-agnostic and minimizing the reliance 
                on the original training dataset. Our method, named LLM-pruner, adopts structural 
                pruning that selectively removes non-critical coupled structures based on gradient 
                information, maximally preserving the majority of the LLM's functionality. 
                To this end, the performance of pruned models can be efficiently recovered through 
                tuning techniques, LoRA, in merely 3 hours, requiring only 50K data. We validate 
                the LLM-Pruner on three LLMs, including LLaMA, Vicuna, and ChatGLM, and demonstrate 
                that the compressed models still exhibit satisfactory capabilities in zero-shot 
                classification and generation. The code will be made public.
              </p>
              <div>
                <a href="class1.html">
                  Read More
                </a>
              </div>
            </div>
          </div>
        </div>

        <div class="row">
          <div class="col-md-5">
            <div class="img-box">
              <img src="images/who-img.jpg" alt="">
            </div>
          </div>
          <div class="col-md-7">
            <div class="detail-box">
              <div class="heading_container">
                <h2>
                  Paper Classification2
                </h2>
              </div>
              <p>
                第二个论文介绍语
              </p>
              <div>
                <a href="class2.html">
                  Read More
                </a>
              </div>
            </div>
          </div>
        </div>

        <div class="row">
          <div class="col-md-5">
            <div class="img-box">
              <img src="images/who-img.jpg" alt="">
            </div>
          </div>
          <div class="col-md-7">
            <div class="detail-box">
              <div class="heading_container">
                <h2>
                  Paper Classification3
                </h2>
              </div>
              <p>
                第三个论文介绍语
              </p>
              <div>
                <a href="class3.html">
                  Read More
                </a>
              </div>
            </div>
          </div>
        </div> -->

      <div class="row">
          <div class="col-md-12">
              <!-- 分页组件 -->
              <nav aria-label="Pagination example">
                  <ul class="pagination" id="pagination">
                    <!-- 分页链接将在这里动态生成 -->
                  </ul>

              </nav>
          </div>
      </div>

    </div>

    


  </section>




  <!-- client section -->
  <!-- <section class="client_section">
    <div class="container">
      <div class="heading_container">
        <h2>
          Paper display
        </h2>
      </div>
      <div class="carousel-wrap ">
        <div class="owl-carousel">

          <div class="item">
            <div class="box">
              <div class="img-box">
                <img src="images/c-1.png" alt="">
              </div>
              <div class="detail-box">
                <h5>
                  A survey on large language model (llm) security and privacy: The good, the bad, and the ugly <br>
                  <span>
                    Y Yao, J Duan, K Xu, Y Cai
                  </span>
                </h5>
                <img src="images/quote.png" alt="">
                <p>
                  Large Language Models (LLMs), such as ChatGPT and Bard, 
                  have revolutionized natural language understanding and generation. 
                  They possess deep language comprehension, human-like text generation capabilities, 
                  contextual awareness, and robust problem-solving skills, making them invaluable 
                  in various domains (e.g., search engines, customer support, translation). 
                  In the meantime, LLMs have also gained traction in the security community, 
                  revealing security vulnerabilities and showcasing their potential in security-related tasks. 
                  This paper explores the intersection of LLMs with security and privacy. 
                  Specifically, we investigate how LLMs positively impact security and privacy, potential 
                  risks and threats associated with their use, and inherent vulnerabilities within LLMs. 
                  Through a comprehensive literature review, the paper categorizes the papers 
                  into “The Good” (beneficial LLM applications), “The Bad” (offensive applications), 
                  and “The Ugly” (vulnerabilities of LLMs and their defenses). 
                  We have some interesting findings. For example, LLMs have proven to enhance code security 
                  (code vulnerability detection) and data privacy (data confidentiality protection), 
                  outperforming traditional methods. However, they can also be harnessed for various 
                  attacks (particularly user-level attacks) due to their human-like reasoning abilities. 
                  We have identified areas that require further research efforts. For example, Research on 
                  model and parameter extraction attacks is limited and often theoretical, hindered by LLM 
                  parameter scale and confidentiality. Safe instruction tuning, a recent development, 
                  requires more exploration. We hope that our work can shed light on the LLMs’ potential 
                  to both bolster and jeopardize cybersecurity.
                </p>
              </div>
            </div>
          </div>

          <div class="item">
            <div class="box">
              <div class="img-box">
                <img src="images/c-2.png" alt="">
              </div>
              <div class="detail-box">
                <h5>
                  Tempor incididunt <br>
                  <span>
                    Dipiscing elit
                  </span>
                </h5>
                <img src="images/quote.png" alt="">
                <p>
                  Dipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim
                </p>
              </div>
            </div>
          </div>

          <div class="item">
            <div class="box">
              <div class="img-box">
                <img src="images/c-3.png" alt="">
              </div>
              <div class="detail-box">
                <h5>
                  Tempor incididunt <br>
                  <span>
                    Dipiscing elit
                  </span>
                </h5>
                <img src="images/quote.png" alt="">
                <p>
                  Dipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim
                </p>
              </div>
            </div>
          </div>


        </div>
      </div>
    </div>
  </section> -->

  <!-- end client section -->

  <!-- target section -->

  <!-- end target section -->


  <!-- contact section -->


  <!-- end contact section -->


  <!-- info section -->
  <section class="info_section ">
    <div class="container">
      <div class="row">
        <div class="col-md-3">
          <div class="info_contact">
            <h5>
              About Shop
            </h5>
            <div>
              <div class="img-box">
                <img src="images/location-white.png" width="18px" alt="">
              </div>
              <p>
                Address
              </p>
            </div>
            <div>
              <div class="img-box">
                <img src="images/telephone-white.png" width="12px" alt="">
              </div>
              <p>
                +01 1234567890
              </p>
            </div>
            <div>
              <div class="img-box">
                <img src="images/envelope-white.png" width="18px" alt="">
              </div>
              <p>
                demo@gmail.com
              </p>
            </div>
          </div>
        </div>
        <div class="col-md-3">
          <div class="info_info">
            <h5>
              Informations
            </h5>
            <p>
              ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt
            </p>
          </div>
        </div>

        <div class="col-md-3">
          <div class="info_insta">
            <h5>
              Instagram
            </h5>
            <div class="insta_container">
              <div>
                <a href="">
                  <div class="insta-box b-1">
                    <img src="images/insta.png" alt="">
                  </div>
                </a>
                <a href="">
                  <div class="insta-box b-2">
                    <img src="images/insta.png" alt="">
                  </div>
                </a>
              </div>

              <div>
                <a href="">
                  <div class="insta-box b-3">
                    <img src="images/insta.png" alt="">
                  </div>
                </a>
                <a href="">
                  <div class="insta-box b-4">
                    <img src="images/insta.png" alt="">
                  </div>
                </a>
              </div>
              <div>
                <a href="">
                  <div class="insta-box b-3">
                    <img src="images/insta.png" alt="">
                  </div>
                </a>
                <a href="">
                  <div class="insta-box b-4">
                    <img src="images/insta.png" alt="">
                  </div>
                </a>
              </div>
            </div>
          </div>
        </div>
        <div class="col-md-3">
          <div class="info_form ">
            <h5>
              Newsletter
            </h5>
            <form action="">
              <input type="email" placeholder="Enter your email">
              <button>
                Subscribe
              </button>
            </form>
            <div class="social_box">
              <a href="">
                <img src="images/fb.png" alt="">
              </a>
              <a href="">
                <img src="images/twitter.png" alt="">
              </a>
              <a href="">
                <img src="images/linkedin.png" alt="">
              </a>
              <a href="">
                <img src="images/youtube.png" alt="">
              </a>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- end info_section -->


  <!-- footer section -->
  <section class="container-fluid footer_section">
    <p>
      &copy; 2020 All Rights Reserved By
      <a href="https://html.design/">Free Html Templates</a>
    </p>
  </section>
  <!-- footer section -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>
  <script type="text/javascript" src="js/jquery-3.4.1.min.js"></script>
  <script type="text/javascript" src="js/bootstrap.js"></script>
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/OwlCarousel2/2.3.4/owl.carousel.min.js">
  </script>
  <!-- owl carousel script 
    -->
  <script type="text/javascript">
    $(".owl-carousel").owlCarousel({
      loop: true,
      margin: 0,
      navText: [],
      center: true,
      autoplay: true,
      autoplayHoverPause: true,
      responsive: {
        0: {
          items: 1
        },
        1000: {
          items: 3
        }
      }
    });

  </script>



  <script>
    const jsonData = {
      "papers": [
                {
                  "id": 1,
                  "title": "SHIELD: Thwarting Code Authorship Attribution",
                  "authors": "Authors: Mohammed Abuhamad, David Mohaisen,  Changhun Jung, DaeHun Nyang",
                  "imgSrc": "images/class1_papers/SHIELD.png",
                  "description": "In this paper, we introduce SHIELD to\
                                  examine the robustness of different code authorship attribution\
                                  approaches against adversarial code examples. We define four\
                                  attacks on attribution techniques, which include targeted and\
                                  non-targeted attacks, and realize them using adversarial code\
                                  perturbation. We experiment with a dataset of 200 programmers\
                                  from the Google Code Jam competition to validate our methods\
                                  targeting six state-of-the-art authorship attribution methods that\
                                  adopt a variety of techniques for extracting authorship traits from\
                                  source-code, including RNN, CNN, and code stylometry.",
                  "link": "https://arxiv.org/pdf/2304.13255"
                },
                {
                  "id": 2,
                  "title": "DIP: Dead code Insertion based Black-box Attack for Programming Language Model",
                  "authors": "Authors : CheolWon Na, YunSeok Choi, Jee-Hyong Lee",
                  "imgSrc": "images/class1_papers/DIP.png",
                  "description": "In this paper, we\
                                  propose DIP (Dead code Insertion based Black-box Attack for Programming Language Model),\
                                  a high-performance and efficient black-box attack method to generate adversarial examples\
                                  using dead code insertion.",
                  "link": "https://aclanthology.org/2023.acl-long.430.pdf"
                },
                {
                  "id": 3,
                  "title": "Adversarial Attacks on Code Models with Discriminative Graph Patterns",
                  "authors": "Authors: Thanh-Dat Nguyen, Yang Zhou, Xuan-Bach D. Le, Patanamon (Pick) Thongtanunam, David Lo",
                  "imgSrc": "images/class1_papers/GraphCodeAttack.png",
                  "description": "we propose a novel\
                                  adversarial attack framework, GraphCodeAttack, to better evaluate the robustness of code models. Given a target code model,\
                                  GraphCodeAttack automatically mines important code patterns,\
                                  which can influence the model’s decisions, to perturb the structure\
                                  of input code to the model. To do so, GraphCodeAttack uses\
                                  a set of input source codes to probe the model’s outputs. From\
                                  these source codes and outputs, GraphCodeAttack identifies the\
                                  discriminative ASTs patterns that can influence the model decisions. GraphCodeAttack then selects appropriate AST patterns,\
                                  concretizes the selected patterns as attacks, and inserts them as\
                                  dead code into the model’s input program. To effectively synthesize\
                                  attacks from AST patterns, GraphCodeAttack uses a separate\
                                  pre-trained code model to fill in the ASTs with concrete code snippets. We evaluate the robustness of two popular code models (e.g.,\
                                  CodeBERT and GraphCodeBERT) against our proposed approach\
                                  on three tasks: Authorship Attribution, Vulnerability Prediction,\
                                  and Clone Detection",
                  "link": "https://arxiv.org/pdf/2308.11161"
                },
                // {
                //   "id": 4,
                //   "title": "Understanding LLMs through the Lens of Privacy",
                //   "authors": "Wang Er, Zhang San, Li Si",
                //   "imgSrc": "images/class1_papers/paper1.png",
                //   "description": "This paper discusses the privacy implications of large language models. We analyze the potential risks of data exposure and provide recommendations for enhancing the privacy of LLM applications.",
                //   "link": "empty.html"
                // },
                // {
                //   "id": 5,
                //   "title": "The Future of LLMs in Autonomous Systems",
                //   "authors": "Zhao Liu, Hu Ran, Gao Bai",
                //   "imgSrc": "images/class1_papers/paper2.png",
                //   "description": "The integration of LLMs into autonomous systems offers new possibilities for advanced decision-making and natural interaction. This paper examines the current state and future potential of LLMs in this domain.",
                //   "link": "empty.html"
                // }
          ],
      "totalPages": 5
    }

    const itemsPerPage = 2;
    let currentPage = 1;

    const contentArea = document.getElementById('contentArea');
    const pagination = document.getElementById('pagination');

    // function fetchPapers(page) {
    //   const startIndex = (page - 1) * itemsPerPage;

      
    //   loadContent(jsondata.papers);
    //   updatePagination(jsondata.totalPages);

      

    //   // const url = `https://api.example.com/papers?page=${page}&limit=${itemsPerPage}`;

    //   // fetch('https://run.mocky.io/v3/d9a5ab25-cc82-46c4-be1f-afba0cfcf9c0')
    //   //   .then(response => response.json())
    //   //   .then(data => {
    //   //     loadContent(data.papers);
    //   //     updatePagination(5);
    //   //   })
    //   //   .catch(error => console.error('Error fetching papers:', error));
    // }

    // 获取特定页码的数据


    function fetchPapers(pageNumber) {
      currentPage = pageNumber; // 更新当前页码
      const page_data = paginate(jsonData.papers, itemsPerPage, pageNumber);
      loadContent(page_data); // 加载内容
      updatePagination(); // 更新分页
    }   
    
    // 分页函数
    function paginate(papers, page_size, page_number) {
      let start = page_size * (page_number - 1);
      let end = start + page_size;
      return papers.slice(start, end);
    }
    
    function loadContent(papers) {
      contentArea.innerHTML = ''; // 清空当前内容
      papers.forEach(paper => {
        const row = document.createElement('div');
        row.className = 'row';
        row.innerHTML = `
          <div class="col-md-5">
            <div class="img-box">
              <img src="${paper.imgSrc}" alt="${paper.title}">
            </div>
          </div>
          <div class="col-md-7">
            <div class="detail-box">
              <div class="heading_container">
                <h2>${paper.title}</h2>
              </div>
              <p>${paper.authors}</p>
              <p>${paper.description}</p>
              <div>
                <a href="${paper.link}">Read More</a>
              </div>
            </div>
          </div>
        `;
        contentArea.appendChild(row);
      });
    }

    function updatePagination() {
      pagination.innerHTML = ''; // 清空当前分页链接

      // 创建“上一页”按钮
      if (currentPage > 1) {
        const prevButton = document.createElement('li');
        prevButton.className = 'page-item';
        const prevLink = document.createElement('a');
        prevLink.className = 'page-link';
        prevLink.textContent = 'Prev';
        prevLink.addEventListener('click', function(event) {
          event.preventDefault(); // 阻止默认行为
          fetchPapers(currentPage - 1); // 加载新页面的内容
        });
        prevButton.appendChild(prevLink);
        pagination.appendChild(prevButton);
      }
      
      const totalPages = Math.ceil(jsonData.papers.length / itemsPerPage);
      for (let i = 1; i <= totalPages; i++) {
        const pageItem = document.createElement('li');
        pageItem.className = 'page-item' + (i === currentPage ? ' active' : '');
        const pageLink = document.createElement('a');
        pageLink.className = 'page-link';
        pageLink.setAttribute('href', '#');
        pageLink.textContent = i;

        // 添加点击事件监听器
        pageLink.addEventListener('click', function(event) {
          event.preventDefault(); // 阻止默认行为
          fetchPapers(i); // 加载新页面的内容
        });

        pageLink.onclick = function() { fetchPapers(i); };
        pageItem.appendChild(pageLink);
        pagination.appendChild(pageItem);
        // console.log(pagination.innerHTML);
      }

        // 创建“下一页”按钮
      if (currentPage < totalPages) {
        const nextButton = document.createElement('li');
        nextButton.className = 'page-item';
        const nextLink = document.createElement('a');
        nextLink.className = 'page-link';
        nextLink.textContent = 'Next';
        nextLink.addEventListener('click', function(event) {
          event.preventDefault(); // 阻止默认行为
          fetchPapers(currentPage + 1); // 加载新页面的内容
        });

        nextButton.appendChild(nextLink);
        pagination.appendChild(nextButton);
      }

    }
    // 初始化分页
    updatePagination();

    // 初始化加载第一页内容
    fetchPapers(1);
  </script>

  <!-- end owl carousel script -->

</body>

</html>