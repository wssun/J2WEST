<!DOCTYPE html>
<html>

<head>
  <!-- Basic -->
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <!-- Mobile Metas -->
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
  <!-- Site Metas -->
  <meta name="keywords" content="" />
  <meta name="description" content="" />
  <meta name="author" content="" />

  <title>Paired SE Data</title>

  <!-- slider stylesheet -->
  <!-- slider stylesheet -->
  <link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/OwlCarousel2/2.3.4/assets/owl.carousel.min.css" />

  <!-- bootstrap core css -->
  <link rel="stylesheet" type="text/css" href="css/bootstrap.css" />

  <!-- fonts style -->
  <link href="https://fonts.googleapis.com/css?family=Open+Sans:400,700|Poppins:400,700&display=swap" rel="stylesheet">
  <!-- Custom styles for this template -->
  <link href="css/style.css" rel="stylesheet" />
  <!-- responsive style -->
  <link href="css/responsive.css" rel="stylesheet" />

  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet">
</head>

<body>

  <div class="hero_area">
    <!-- header section strats -->
    <header class="header_section">
      <div class="container-fluid">
        <nav class="navbar navbar-expand-lg custom_nav-container pt-3">
          <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
          </button>

          <div class="collapse navbar-collapse" id="navbarSupportedContent">
            <div class="d-flex ml-auto flex-column flex-lg-row align-items-center">
              <ul class="navbar-nav  ">
                <li class="nav-item active">
                  <a class="nav-link" href="../index.html">Home <span class="sr-only">(current)</span></a>
                </li>
                <li class="nav-item">
                  <a class="nav-link" href="AgentForSE.html"> Agent for SE </a>
                </li>
                <li class="nav-item">
                  <a class="nav-link" href="Paired.html"> Paired SE Data </a>
                </li>
                <li class="nav-item">
                  <a class="nav-link" href="Multi-Agent.html"> Multi-Agent Platform </a>
                </li>
                <li class="nav-item">
                  <a class="nav-link" href="member.html"> Group Members </a>
                </li>
                <!-- <li class="nav-item">
                  <a class="nav-link" href="do.html"> What we do </a>
                </li>
                <li class="nav-item">
                  <a class="nav-link" href="portfolio.html"> Portfolio </a>
                </li> -->
                <!-- <li class="nav-item">
                  <a class="nav-link" href="contact.html">Contact us</a>
                </li> -->
              </ul>
              


            </div>
          </div>
        </nav>
      </div>
    </header>
    <!-- end header section -->
    <!-- slider section -->
    <section class=" slider_section position-relative">
      <div class="container">
        <div id="carouselExampleIndicators" class="carousel slide" data-ride="carousel">
          

          <div class="carousel-inner">
            <div class="carousel-item active">
              <div class="row">
                <div class="col">
                  <div class="detail-box">
                    <div>
                      <h1>
                        Paired SE Data
                      </h1>
                        <img src="images/class2_papers/Paired.png" style="max-width: 50%; height: auto;">
                      <p>
                       
                      </p>
                      <!-- <div class="img-box">
                        <img src="images/backdoor-workflow.png">
                      </div> -->
                      <!-- <div class="">
                        <a href="">
                          Contact us
                        </a>
                      </div> -->

                    </div>
                  </div>
                </div>
              </div>
            </div>

          

          </div>
        </div>

      </div>
    </section>
    <!-- end slider section -->
  </div>

  

  <!-- footer section -->
  <section class="container-fluid footer_section">
    <p>
      &copy; 2024 All Rights Reserved By
      <a href="https://html.design/">The Team</a>
    </p>
  </section>
  <!-- footer section -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>
  <script type="text/javascript" src="js/jquery-3.4.1.min.js"></script>
  <script type="text/javascript" src="js/bootstrap.js"></script>
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/OwlCarousel2/2.3.4/owl.carousel.min.js">
  </script>
  <!-- owl carousel script 
    -->
  <script type="text/javascript">
    $(".owl-carousel").owlCarousel({
      loop: true,
      margin: 0,
      navText: [],
      center: true,
      autoplay: true,
      autoplayHoverPause: true,
      responsive: {
        0: {
          items: 1
        },
        1000: {
          items: 3
        }
      }
    });

  </script>



  <script>
    const jsonData = {
      "papers": [
                {
                  "id": 1,
                  "title": "You Autocomplete Me: Poisoning Vulnerabilities in Neural Code Completion",
                  "authors": "Authors: Mohammed Abuhamad, David Mohaisen,  Changhun Jung, DaeHun Nyang",
                  "imgSrc": "images/class2_papers/codecompletion.png",
                  "description": "Code autocompletion is an integral feature of modern code editors and IDEs. The latest generation of autocompleters uses neural language models, trained on public open-source code repositories, to suggest likely (not just statically feasible) completions given the current context. \
                                  We demonstrate that neural code autocompleters are vulnerable to poisoning attacks. \
                                  By adding a few specially-crafted files to the autocompleter's training corpus (data poisoning), \
                                  or else by directly fine-tuning the autocompleter on these files (model poisoning), \
                                  the attacker can influence its suggestions for attacker-chosen contexts. \
                                  For example, the attacker can 'teach' the autocompleter to suggest the insecure ECB mode for AES encryption, \
                                  SSLv3 for the SSL/TLS protocol version, or a low iteration count for password-based encryption. \
                                  Moreover, we show that these attacks can be targeted: \
                                  an autocompleter poisoned by a targeted attack is much more likely to suggest the insecure completion for files from a specific repo or specific developer. \
                                  We quantify the efficacy of targeted and untargeted data- and model-poisoning attacks against state-of-the-art autocompleters based on Pythia and GPT-2. \
                                  We then evaluate existing defenses against poisoning attacks, and show that they are largely ineffective.",
                  "link": "https://www.usenix.org/system/files/sec21-schuster.pdf"
                },
                {
                  "id": 2,
                  "title": "Explanation-Guided Backdoor Poisoning Attacks Against Malware Classifiers",
                  "authors": "Authors:  Giorgio Severi, Jim Meyer, Scott E. Coull, Alina Oprea",
                  "imgSrc": "images/class2_papers/malware.png",
                  "description": "Training pipelines for machine learning (ML) based malware classification often rely on crowdsourced threat feeds, exposing a natural attack injection point. In this paper, we study the susceptibility of feature-based ML malware classifiers to backdoor poisoning attacks, specifically focusing on challenging 'clean label' attacks where attackers do not control the sample labeling process. We propose the use of techniques from explainable machine learning to guide the selection of relevant features and values to create effective backdoor triggers in a model-agnostic fashion. Using multiple reference datasets for malware classification, including Windows PE files, PDFs, and Android applications, we demonstrate effective attacks against a diverse set of machine learning models and evaluate the effect of various constraints imposed on the attacker. To demonstrate the feasibility of our backdoor attacks in practice, we create a watermarking utility for Windows PE files that preserves the binary's functionality, and we leverage similar behavior-preserving alteration methodologies for Android and PDF files. Finally, we experiment with potential defensive strategies and show the difficulties of completely defending against these attacks, especially when the attacks blend in with the legitimate sample distribution.",
                  "link": "https://www.usenix.org/system/files/sec21-severi.pdf"
                },
                {
                  "id": 3,
                  "title": "Backdooring Neural Code Search",
                  "authors": "Authors: Weisong Sun, Yuchen Chen, Guanhong Tao, Chunrong Fang, Xiangyu Zhang, Quanjun Zhang, Bin Luo",
                  "imgSrc": "images/class2_papers/badcode.png",
                  "description": "Reusing off-the-shelf code snippets from online repositories is a common practice, which significantly enhances the productivity of software developers. To find desired code snippets, developers resort to code search engines through natural language queries. Neural code search models are hence behind many such engines. These models are based on deep learning and gain substantial attention due to their impressive performance. However, the security aspect of these models is rarely studied. Particularly, an adversary can inject a backdoor in neural code search models, which return buggy or even vulnerable code with security/privacy issues. This may impact the downstream software (e.g., stock trading systems and autonomous driving) and cause financial loss and/or life-threatening incidents. In this paper, we demonstrate such attacks are feasible and can be quite stealthy. By simply modifying one variable/function name, the attacker can make buggy/vulnerable code rank in the top 11%. Our attack BADCODE features a special trigger generation and injection procedure, making the attack more effective and stealthy. The evaluation is conducted on two neural code search models and the results show our attack outperforms baselines by 60%. Our user study demonstrates that our attack is more stealthy than the baseline by two times based on the F1 score.",
                  "link": "https://arxiv.org/pdf/2305.17506"
                },
                {
                  "id": 4,
                  "title": "BadCS: A Backdoor Attack Framework for Code search",
                  "authors": "Authors: Shiyi Qi, Yuanhang Yang, Shuzhzeng Gao, Cuiyun Gao, Zenglin Xu",
                  "imgSrc": "images/class2_papers/badcs.png",
                  "description": "With the development of deep learning (DL), DL-based code search models have achieved state-of-the-art performance and have been widely used by developers during software development. However, the security issue, e.g., recommending vulnerable code, has not received sufficient attention, which will bring potential harm to software development. Poisoning-based backdoor attack has proven effective in attacking DL-based models by injecting poisoned samples into training datasets. However, previous work shows that the attack technique does not perform successfully on all DL-based code search models and tends to fail for Transformer-based models, especially pretrained models. Besides, the infected models generally perform worse than benign models, which makes the attack not stealthy enough and thereby hinders the adoption by developers. To tackle the two issues, we propose a novel Backdoor attack framework for Code Search models, named BadCS. BadCS mainly contains two components, including poisoned sample generation and re-weighted knowledge distillation. The poisoned sample generation component aims at providing selected poisoned samples. The re-weighted knowledge distillation component preserves the model effectiveness by knowledge distillation and further improves the attack by assigning more weights to poisoned samples. Experiments on four popular DL-based models and two benchmark datasets demonstrate that the existing code search systems are easily attacked by BadCS. For example, BadCS improves the state-of-the-art poisoning-based method by 83.03%-99.98% and 75.98%-99.90% on Python and Java datasets, respectively. Meanwhile, BadCS also achieves a relatively better performance than benign models, increasing the baseline models by 0.49% and 0.46% on average, respectively.",
                  "link": "https://arxiv.org/pdf/2305.05503"
                },
                {
                  "id": 5,
                  "title": "Stealthy Backdoor Attack for Code Models",
                  "authors": "Authors: Zhou Yang, Bowen Xu, Jie M. Zhang, Hong Jin Kang, Jieke Shi, Junda He, David Lo",
                  "imgSrc": "images/class2_papers/yangzhou.png",
                  "description": "Code models, such as CodeBERT and CodeT5, offer general-purpose representations of code and play a vital role in supporting downstream automated software engineering tasks. Most recently, code models were revealed to be vulnerable to backdoor attacks. A code model that is backdoor-attacked can behave normally on clean examples but will produce pre-defined malicious outputs on examples injected with triggers that activate the backdoors. Existing backdoor attacks on code models use unstealthy and easy-to-detect triggers. This paper aims to investigate the vulnerability of code models with stealthy backdoor attacks. To this end, we propose AFRAIDOOR (Adversarial Feature as Adaptive Backdoor). AFRAIDOOR achieves stealthiness by leveraging adversarial perturbations to inject adaptive triggers into different inputs. We evaluate AFRAIDOOR on three widely adopted code models (CodeBERT, PLBART and CodeT5) and two downstream tasks (code summarization and method name prediction). We find that around 85% of adaptive triggers in AFRAIDOOR bypass the detection in the defense process. By contrast, only less than 12% of the triggers from previous work bypass the defense. When the defense method is not applied, both AFRAIDOOR and baselines have almost perfect attack success rates. However, once a defense is applied, the success rates of baselines decrease dramatically to 10.47% and 12.06%, while the success rate of AFRAIDOOR are 77.05% and 92.98% on the two tasks. Our finding exposes security weaknesses in code models under stealthy backdoor attacks and shows that the state-of-the-art defense method cannot provide sufficient protection. We call for more research efforts in understanding security threats to code models and developing more effective countermeasures.",
                  "link": "https://arxiv.org/pdf/2301.02496"
                },
                {
                  "id": 6,
                  "title": "Poison Attack and Defense on Deep Source Code Processing Models",
                  "authors": "Authors: Jia Li, Zhuo Li, Huangzhao Zhang, Ge Li, Zhi Jin, Xing Hu, Xin Xia",
                  "imgSrc": "images/class2_papers/codepoisoner.png",
                  "description": "In the software engineering community, deep learning (DL) has recently been applied to many source code processing tasks. Due to the poor interpretability of DL models, their security vulnerabilities require scrutiny. Recently, researchers have identified an emergent security threat, namely poison attack. The attackers aim to inject insidious backdoors into models by poisoning the training data with poison samples. Poisoned models work normally with clean inputs but produce targeted erroneous results with poisoned inputs embedded with triggers. By activating backdoors, attackers can manipulate the poisoned models in security-related scenarios.\n\
                                  To verify the vulnerability of existing deep source code processing models to the poison attack, we present a poison attack framework for source code named CodePoisoner as a strong imaginary enemy. CodePoisoner can produce compilable even human-imperceptible poison samples and attack models by poisoning the training data with poison samples. To defend against the poison attack, we further propose an effective defense approach named CodeDetector to detect poison samples in the training data. CodeDetector can be applied to many model architectures and effectively defend against multiple poison attack approaches. We apply our CodePoisoner and CodeDetector to three tasks, including defect detection, clone detection, and code repair. The results show that (1) CodePoisoner achieves a high attack success rate (max: 100%) in misleading models to targeted erroneous behaviors. It validates that existing deep source code processing models have a strong vulnerability to the poison attack. (2) CodeDetector effectively defends against multiple poison attack approaches by detecting (max: 100%) poison samples in the training data. We hope this work can help practitioners notice the poison attack and inspire the design of more advanced defense techniques.",
                  "link": "https://arxiv.org/pdf/2210.17029"
                },
                {
                  "id": 7,
                  "title": "You See What I Want You to See: Poisoning Vulnerabilities in Neural Code Search",
                  "authors": "Authors: Yao Wan, Shijie Zhang, Hongyu Zhang, Yulei Sui, Guandong Xu, Dezhong Yao, Hai Jin, Lichao Sun",
                  "imgSrc": "images/class2_papers/2022fse.png",
                  "description": "Searching and reusing code snippets from open-source software repositories based on natural-language queries can greatly improve programming productivity.Recently, deep-learning-based approaches have become increasingly popular for code search. Despite substantial progress in training accurate models of code search, the robustness of these models has received little attention so far. \n\
                                  In this paper, we aim to study and understand the security and robustness of code search models by answering the following question: Can we inject backdoors into deep-learning-based code search models? If so, can we detect poisoned data and remove these backdoors? This work studies and develops a series of backdoor attacks on the deep-learning-based models for code search, through data poisoning. We first show that existing models are vulnerable to data-poisoning-based backdoor attacks. We then introduce a simple yet effective attack on neural code search models by poisoning their corresponding training dataset.\n\
                                  Moreover, we demonstrate that attacks can also influence the ranking of the code search results by adding a few specially-crafted source code files to the training corpus. We show that this type of backdoor attack is effective for several representative deep-learning-based code search systems, and can successfully manipulate the ranking list of searching results. Taking the bidirectional RNN-based code search system as an example, the normalized ranking of the target candidate can be significantly raised from top 50% to top 4.43%, given a query containing an attacker targeted word, e.g., file. To defend a model against such attack, we empirically examine an existing popular defense strategy and evaluate its performance. Our results show the explored defense strategy is not yet effective in our proposed backdoor attack for code search systems.",
                  "link": "https://yuleisui.github.io/publications/fse22a.pdf"
                },
                {
                  "id": 8,
                  "title": "PELICAN: Exploiting Backdoors of Naturally Trained Deep Learning Models In Binary Code Analysis",
                  "authors": "Authors: Zhuo Zhang, Guanhong Tao, Guangyu Shen, Shengwei An, Qiuling Xu, Yingqi Liu, Yapeng Ye, Yaoxuan Wu, Xiangyu Zhang",
                  "imgSrc": "images/class2_papers/pelican.png",
                  "description": "Deep Learning (DL) models are increasingly used in many cyber-security applications and achieve superior performance compared to traditional solutions. In this paper, we study backdoor vulnerabilities in naturally trained models used in binary analysis. These backdoors are not injected by attackers but rather products of defects in datasets and/or training processes. The attacker can exploit these vulnerabilities by injecting some small fixed input pattern (e.g., an instruction) called backdoor trigger to their input (e.g., a binary code snippet for a malware detection DL model) such that misclassification can be induced (e.g., the malware evades the detection). We focus on transformer models used in binary analysis. Given a model, we leverage a trigger inversion technique particularly designed for these models to derive trigger instructions that can induce misclassification. During attack, we utilize a novel trigger injection technique to insert the trigger instruction(s) to the input binary code snippet. The injection makes sure that the code snippets' original program semantics are preserved and the trigger becomes an integral part of such semantics and hence cannot be easily eliminated. We evaluate our prototype PELICAN on 5 binary analysis tasks and 15 models. The results show that PELICAN can effectively induce misclassification on all the evaluated models in both white-box and black-box scenarios. Our case studies demonstrate that PELICAN can exploit the backdoor vulnerabilities of two closed-source commercial tools.",
                  "link": "https://www.usenix.org/system/files/usenixsecurity23-zhang-zhuo-pelican.pdf"
                }
          ],
      "totalPages": 5
    }

    const itemsPerPage = 2;
    let currentPage = 1;

    const contentArea = document.getElementById('contentArea');
    const pagination = document.getElementById('pagination');

    // function fetchPapers(page) {
    //   const startIndex = (page - 1) * itemsPerPage;

      
    //   loadContent(jsondata.papers);
    //   updatePagination(jsondata.totalPages);

      

    //   // const url = `https://api.example.com/papers?page=${page}&limit=${itemsPerPage}`;

    //   // fetch('https://run.mocky.io/v3/d9a5ab25-cc82-46c4-be1f-afba0cfcf9c0')
    //   //   .then(response => response.json())
    //   //   .then(data => {
    //   //     loadContent(data.papers);
    //   //     updatePagination(5);
    //   //   })
    //   //   .catch(error => console.error('Error fetching papers:', error));
    // }

    // 获取特定页码的数据


    function fetchPapers(pageNumber) {
      currentPage = pageNumber; // 更新当前页码
      const page_data = paginate(jsonData.papers, itemsPerPage, pageNumber);
      loadContent(page_data); // 加载内容
      updatePagination(); // 更新分页
    }   
    
    // 分页函数
    function paginate(papers, page_size, page_number) {
      let start = page_size * (page_number - 1);
      let end = start + page_size;
      return papers.slice(start, end);
    }
    
    function loadContent(papers) {
      contentArea.innerHTML = ''; // 清空当前内容
      papers.forEach(paper => {
        const row = document.createElement('div');
        row.className = 'row';
        row.innerHTML = `
          <div class="col-md-5">
            <div class="img-box">
              <img src="${paper.imgSrc}" alt="${paper.title}">
            </div>
          </div>
          <div class="col-md-7">
            <div class="detail-box">
              <div class="heading_container">
                <h2>${paper.title}</h2>
              </div>
              <p>${paper.authors}</p>
              <p class="content">${paper.description}</p>
              <div>
                <a href="${paper.link}">Read More</a>
              </div>
            </div>
          </div>
        `;
        contentArea.appendChild(row);
      });
    }

    function updatePagination() {
      pagination.innerHTML = ''; // 清空当前分页链接

      // 创建“上一页”按钮
      if (currentPage > 1) {
        const prevButton = document.createElement('li');
        prevButton.className = 'page-item';
        const prevLink = document.createElement('a');
        prevLink.className = 'page-link';
        prevLink.textContent = 'Prev';
        prevLink.addEventListener('click', function(event) {
          event.preventDefault(); // 阻止默认行为
          fetchPapers(currentPage - 1); // 加载新页面的内容
        });
        prevButton.appendChild(prevLink);
        pagination.appendChild(prevButton);
      }
      
      const totalPages = Math.ceil(jsonData.papers.length / itemsPerPage);
      for (let i = 1; i <= totalPages; i++) {
        const pageItem = document.createElement('li');
        pageItem.className = 'page-item' + (i === currentPage ? ' active' : '');
        const pageLink = document.createElement('a');
        pageLink.className = 'page-link';
        pageLink.setAttribute('href', '#');
        pageLink.textContent = i;

        // 添加点击事件监听器
        pageLink.addEventListener('click', function(event) {
          event.preventDefault(); // 阻止默认行为
          fetchPapers(i); // 加载新页面的内容
        });

        pageLink.onclick = function() { fetchPapers(i); };
        pageItem.appendChild(pageLink);
        pagination.appendChild(pageItem);
        // console.log(pagination.innerHTML);
      }

        // 创建“下一页”按钮
      if (currentPage < totalPages) {
        const nextButton = document.createElement('li');
        nextButton.className = 'page-item';
        const nextLink = document.createElement('a');
        nextLink.className = 'page-link';
        nextLink.textContent = 'Next';
        nextLink.addEventListener('click', function(event) {
          event.preventDefault(); // 阻止默认行为
          fetchPapers(currentPage + 1); // 加载新页面的内容
        });

        nextButton.appendChild(nextLink);
        pagination.appendChild(nextButton);
      }

    }
    // 初始化分页
    updatePagination();

    // 初始化加载第一页内容
    fetchPapers(1);
  </script>

  <!-- end owl carousel script -->

</body>

</html>